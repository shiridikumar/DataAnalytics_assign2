# -*- coding: utf-8 -*-
"""dataanalytics_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k9uRUG2BiMA2R3hpnIEDrvuj6fLFGnv_
"""

import pandas as pd
import numpy as np
import csv
import os
df=pd.read_csv("master.csv")

def preprocess_data(df):
  df.drop(axis=1,columns=["country-year"],inplace=True)

  df["population"].describe()
  med=df["population"].describe()
  print(med)
  # ma=df["population"].max()
  bins = [0, med["25%"], med["75%"], med["max"]]
  labels=["Low_population", "Medium_population", "High_population"]
  df["population"]=pd.cut(df["population"],bins=bins,labels=labels)

  med=df["suicides_no"].describe()
  print(med)
  bins=[i*10 for i in range((int(med["75%"]))//10+2)]
  print(bins)
  bins.append(int(med["max"])+1)
  labels=["{}-{}".format(bins[i],bins[i+1]) for i in range(len(bins)-1)]
  labels[-1]=">{})".format(bins[-2])
  print(labels)
  df["suicides_no"]=pd.cut(df["suicides_no"],bins=bins,labels=labels)

  med=df[' gdp_for_year ($) '].describe()
  print(med)
  bins = [0, med["25%"], med["75%"], med["max"]]
  labels=["Low_income_country", "Medium_income_country", "High_income_country"]
  # df["population"]=pd.cut(df["population"],bins=bins,labels=labels)

  df[" gdp_for_year ($) "]=pd.cut(df[" gdp_for_year ($) "],bins=bins,labels=labels)

  med=df["gdp_per_capita ($)"].describe()
  print(med)
  bins=[i*1000 for i in range(int(med["75%"]+1000)//1000+1)]
  bins.append(int(med["max"]))
  labels=["{}-{}".format(bins[i],bins[i+1]) for i in range(len(bins)-1)]
  print(bins)
  labels[-1]=">{}".format(bins[-2])
  print(labels)

  df["gdp_per_capita ($)"]=pd.cut(df["gdp_per_capita ($)"],bins=bins,labels=labels)
  return df

df=preprocess_data(df)

df.drop(axis=1,columns=["suicides/100k pop"],inplace=True)
df

df.drop(axis=1,columns=["HDI for year"],inplace=True)
df

"""##***computing cubes***"""

minsup=0.01
dup=df[["country","year","age","sex"," gdp_for_year ($) "]]
dup

def getcounts(data):
  req=data[:,0]
  datacounts=[]
  k=0
  if(len(req)==1):
    return [1]
  for i in range(len(req)-1):
    k+=1
    if(req[i]!=req[i+1]):
      datacounts.append(k)
      k=0
  datacounts.append(k+1)
  return datacounts

"""##***Assuming that the cube fits into main memory***"""

vals=dup.values
vals=vals[vals[:,0].argsort()]
dup.sort_values("country",inplace=True)
s=[]
dic={}
l=[]
def compute(data,l):
  dim=data.shape[1]
  if(dim==0):
    s.append(l.copy())
    s[-1].append(len(data))
    return 
  data=data[data[:,0].argsort()]
  datacounts=getcounts(data)
  k=0
  for i in range(len(datacounts)):
    c=datacounts[i]
    if(c/len(data)>minsup):
      l.append(data[k,0])
      compute(data[k:k+c,1:],l.copy())
      l.pop()
    k+=c
  l.append("ALL")
  compute(data[:,1:],l.copy())
  l.pop()
compute(vals,l)

# for i in range(6000):
  # print(s[i])
print(len(s))
res=pd.DataFrame(s)
col=list(dup.columns)
col.append("Count")
res.columns=col
res

res.to_csv("part2_1.csv")

"""##***out of memory assumption***

##***External merge sort***
"""

def merge_files(filename,fp1,fp2,attr,start,mid,end):
  t1=mid-start+1
  t2=end-mid
  p1=0
  p2=0
  with open(filename, "w") as f:
      reader = csv.writer(f)
      with open(fp1) as f1:
        r1=csv.reader(f1,delimiter=",")
        with open(fp2) as f2:
          r2=csv.reader(f2,delimiter=",")
          cols=next(r1)
          next(r2)
          reader.writerow(cols)
          val1=next(r1)
          val2=next(r2)
          condition=""
          if(str(val1[attr]).isnumeric()):
            condition="float(val1[attr])<=float(val2[attr])"
          else:
            condition="val1[attr]<=val2[attr]"
          while(p1<t1 and p2<t2):
            if(eval(condition)):
              reader.writerow(val1)
              p1+=1
              if(p1>=t1):
                break
              val1=next(r1)
            else:
              reader.writerow(val2)
              p2+=1
              if(p2>=t2):
                break
              val2=next(r2)
          
          while(p1<t1):
            reader.writerow(val1)
            p1+=1
            if(p1>=t1):
              break
            val1=next(r1)
          while(p2<t2):
            reader.writerow(val2)
            p2+=1
            if(p2>=t2):
              break
            val2=next(r2)

rows=830
def external_merge_sort(filename,start,end,attr,columns):
  if(end-start<rows):
    temp=pd.read_csv(filename)
    temp.sort_values(attr,inplace=True)
    temp.set_index(temp.columns[0],inplace=True)
    temp.to_csv(filename)

  else:
    mid=(start+end)//2
    f1=[]
    f2=[]
    x=f1
    with open(filename) as f:
      reader = csv.reader(f, delimiter=",")
      next(reader)
      ind =0
      for i,line in enumerate(reader):
        f1.append(line)
        if(i==mid-start):
          f2.append(line)
          break
      
      for i,line in enumerate(reader):
        f2.append(line)
        
    fp="{}_{}_{}.csv"
    fp1=fp.format(start,mid,attr)
    fp2=fp.format(mid+1,end,attr)
    f1=pd.DataFrame(f1)
    f1.columns=columns
    f1.set_index(f1.columns[0],inplace=True)
    f1.to_csv(fp1)
    external_merge_sort(fp1,start,mid,attr,columns)
    f2=pd.DataFrame(f2)
    f2.columns=columns
    f2.set_index(f2.columns[0],inplace=True)
    f2.to_csv(fp2)
    external_merge_sort(fp2,mid+1,end,attr,columns)
    merge_files(filename,fp1,fp2,list(columns).index(attr),start,mid,end)
    os.remove(fp1)
    os.remove(fp2)

df=pd.read_csv("master.csv")
external_merge_sort("master.csv",0,27819,"population",df.columns)

def getcounts_page(file,attr):
  temp=pd.read_csv(file)
  req=temp[attr]
  datacounts=[]
  k=0
  if(len(req)==1):
    return [1]
  for i in range(len(req)-1):
    k+=1
    if(req[i]!=req[i+1]):
      datacounts.append(k)
      k=0
  datacounts.append(k+1)
  return datacounts

# approxiamting each tuple of a table is of fixed length
# approximating block size to store 10000 entries of a table instead of hardcoding it to x bytes
# so no of row that could be stored in a block = floor(10000/(no of columns))
global block_size
block_size=2 # in (10k entries)

class Table:
  def __init__(self,file):
    self.filename=file
    rowCount=0
    temp=pd.read_csv(self.filename,nrows=1)
    self.numColumns=len(temp.columns)
    self.columns=temp.columns
    with open(self.filename) as f:
      reader = csv.reader(f, delimiter=",")
      for i, line in enumerate(reader):
        rowCount+=1
    self.numRows=rowCount

  def partition(self,attr):
    total=0
    self.numRowsinBlock=block_size*1000//self.numColumns
    print(self.numRowsinBlock)
    print(self.numColumns,self.numRows)
    block=[]
    print(self.numRows)
    external_merge_sort(self.filename,0,self.numRows-2,attr,self.columns)
    c=getcounts_page(self.filename,attr)
    with open(self.filename) as f:
      reader = csv.reader(f,delimiter=",")
      for i in range(len(c)):
        self.write_partition(self.filename,attr,c[i],i,reader)
        self.make_pages(self.filename,attr,c[i],i,reader)
 
  def write_partition(self,filename,attr,count,i,reader):
      if(i==0):
        next(reader)
      with open("{}_{}_{}".format(filename,attr,i)+".csv","w") as f:
        writer=csv.writer(f)
        writer.writerow(self.columns)
        ind=0
        for i ,line in enumerate(reader):
          writer.writerow(line)
          ind+=1
          if(ind==count):
            break
      

  def make_pages(self,filename,attr,count,i,reader):
    block=[]
    filestring="{}_{}_{}".format(filename,attr,i)
    with open(filestring,"r") as f1:
      r1=csv.reader(f1)
      ind=0
      next(r1)
      pageindex=0
      for i,line in enumerate(r1):
        block.append(line)
        if(len(block)==self.numRowsinBlock):
          newpage=Page(block,filestring,pageindex,list(self.columns))
          block=[]
          pageindex+=1
      if(len(block)!=0):
         newpage=Page(block,filestring,pageindex,list(self.columns))
         block=[]


  def getpage(self):
    p=Page(0,self.filename)
    self.curr=0
    return p.read_page()
  
  def getnextpage(self):
    self.curr+=1
    if(self.curr<self.block_no):
      p=Page(self.curr,self.filename)
      return p.read_page()
    else:
      return False

class Page(Table):
  def __init__(self,block,filestring,pageindex,columns):
    self.filename=filestring+"_page"+str(pageindex)
    self.pageindex=pageindex
    temp=pd.DataFrame(block)
    temp.columns=columns
    self.numColumn=len(columns)
    self.numRows=len(block)
    # print(self.numRows,"***")
    temp.set_index(columns[0],inplace=True)
    temp.to_csv(self.filename)

  def read_page(self):
    df=pd.read_csv(self.filename[:-3]+"_"+str(self.pageindex)+".csv")
    return df

tab=Table("master.csv")
tab.partition("country")

"""##***pre processing***"""

def preprocess_population():
  pageindex=0
  page=tab.getpage()
  avg=0
  # print(page)
  while(pageindex<tab.block_no):
    avg+=sum(page["population"])
    page=tab.getnextpage()
    pageindex+=1
  avg=avg/(tab.numRows-1)
  print(avg,tab.numRows)
preprocess_population()

columns=list(df.columns)
external_merge_sort("master.csv",0,27819,columns[0],columns)

def compute(data,l):
  dim=data.shape[1]
  if(dim==0):
    s.append(l.copy())
    s[-1].append(len(data))
    return 
  data=data[data[:,0].argsort()]
  datacounts=getcounts(data)
  k=0
  for i in range(len(datacounts)):
    c=datacounts[i]
    if(c/len(data)>minsup):
      l.append(data[k,0])
      compute(data[k:k+c,1:],l.copy())
      l.pop()
    k+=c
  l.append("ALL")
  compute(data[:,1:],l.copy())
  l.pop()
# compute(vals,l)

df[df.country=="Uzbekistan"].count()